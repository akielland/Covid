# -*- coding: utf-8 -*-
"""INSTK5000_Project_1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1WH2Gw1lqEaSF7jNYJiIIs8Z1gQ_DCVtV
"""

!git clone https://github.com/chonyy/apriori_python.git

# imports
import numpy as np
import math
import pandas as pd
import matplotlib.pyplot as plt
import statsmodels.api as sm
from scipy.special import expit, logit
from statsmodels.tools import add_constant
from IPython.display import display
from apriori_python.apriori import * 
from apriori_python.utils import *
from scipy.stats import gamma, halfgennorm
from scipy.stats import bernoulli
from sklearn.metrics import accuracy_score, f1_score
from sklearn.model_selection import StratifiedKFold
import sklearn.model_selection
from sklearn.utils import resample
from sklearn.linear_model import LogisticRegression

# Upload observation_features.csv to colab in order to run the code
df = pd.read_csv('/observation_features.csv')

"""Task 1a"""

# a class to simulate data
class Simulator:
    def __init__(self, data):
        self.data = data
        
    # numerical values according to a gamma distribution
    def generate_age(self, ind=10, n_samples=None):
        if n_samples is None:
            n_samples = len(self.data[:,ind])
        hist,bins,_= plt.hist(self.data[:,ind], bins=500)
        plt.close()
        mean = int(bins[np.argmax(hist)])
        std = np.std(self.data[np.where(self.data[:,ind] > mean),ind])
        return (gamma.rvs(a = 1.99, loc=0, scale=std, size=n_samples))
    
    # generating binary values according to a bernoulli distribution
    def generate_binary(self, ind, n_samples=None):
        if n_samples is None:
            n_samples = len(self.data[:,ind])
        return bernoulli.rvs(np.mean(self.data[:,ind]), size=n_samples)
    
    # generating income values (float) accorfing to a half normal distribution
    def generate_income(self, ind=12, n_samples=None):
        if n_samples is None:
            n_samples = len(self.data[:,ind])
        return halfgennorm.rvs(beta=1, scale=np.std(self.data[:,ind]), size=n_samples)
        
    # generating a target vector based on a given target column, feature and effect size
    def add_effect(self, cause_col, target_mean, effect_prob):
        n_samples = len(cause_col)
        cause_frequency = np.mean(cause_col)
        baseline_prob = max([(target_mean - cause_frequency*effect_prob)/(1-cause_frequency),0])
        affected = bernoulli.rvs(effect_prob, size=n_samples)
        non_affected = bernoulli.rvs(baseline_prob, size=n_samples)
        return np.where(cause_col == 1, affected, non_affected)
    
    # generating a target vector based on treatment combinations and effect size for each treatment
    def treatment_effect(self, treatment_cols, target_mean, effect_prob1, effect_prob2):
        both = np.logical_and(treatment_cols[:,0] ==1, treatment_cols[:,1] == 1)
        n_samples = len(treatment_cols[:,0])
        f1 = (np.sum(treatment_cols[:,0]) - np.sum(both))/n_samples
        f2 = (np.sum(treatment_cols[:,1]) - np.sum(both))/n_samples
        f_both = np.mean(both)
        f0 = 1 - f1 - f2 - f_both
        prob_both = (2*effect_prob1 + effect_prob2)/3
        baseline_prob = max([(target_mean - f1*effect_prob1 - f2*effect_prob2 - f_both*prob_both)/(f0), 0])
        affected1 = bernoulli.rvs(effect_prob1, size=n_samples)
        affected2 = bernoulli.rvs(effect_prob2, size=n_samples)
        affected_both = bernoulli.rvs(prob_both, size=n_samples)
        non_affected = bernoulli.rvs(baseline_prob, size=n_samples)
        first = np.where(treatment_cols[:,0] == 1, affected1, non_affected)
        second = np.where(treatment_cols[:,1] == 1, affected2, first)
        return np.where(both == 1, affected_both, second)
    
# transforming the age feature to two binary features, split at 55
def preprocess_age(curr_col):
    above_55 = np.where(curr_col>=55, np.ones(len(curr_col)),
                        np.zeros(len(curr_col))).reshape(len(curr_col),1)
    under_55 = np.where(curr_col<55, np.ones(len(curr_col)),
                        np.zeros(len(curr_col))).reshape(len(curr_col),1)
    return np.concatenate((under_55, above_55), axis=1)
        
# transforms the features to a list of items as preproccessing step for Apriori
def binary_to_str(feature_mat):
    item_list = []
    for row in range(len(feature_mat)):
        curr_list = []
        for feature in range(len(feature_mat[row])):
            if feature_mat[row, feature] == 1:
                curr_list.append(feature)
        item_list.append(curr_list)
    return item_list

# f1 score
def evaluate_f1(clf, X_train, y_train, X_test, y_test):
    clf.fit(X_train, y_train)
    y_pred = clf.predict(X_test)
    return f1_score(y_pred, y_test)

# accuracy
def evaluate_acc(clf, X_train, y_train, X_test, y_test):
    clf.fit(X_train, y_train)
    y_pred = clf.predict(X_test)
    accuracy = accuracy_score(y_pred, y_test)
    return accuracy

# K-fold cross validation using Lasso logistic regression
def kfold_cross_validation(data_features, data_labels, params, k=5):
  skf = StratifiedKFold(n_splits=k)
  n_params = len(params)
  validation_res = np.zeros(n_params)
  for i in range(n_params):
    for train_index, test_index in skf.split(data_features, data_labels):
        X_train, X_test = data_features[train_index], data_features[test_index]
        y_train, y_test = data_labels[train_index], data_labels[test_index]
        validation_res[i] += evaluate_f1(LogisticRegression(penalty='l1', C=params[i], solver='liblinear', random_state=0), X_train, y_train, X_test, y_test)
  return validation_res/k

# pipeline for train-test split, k-fold cross validation, evaluation on test set
# returns a model trained on the entire dataset (including the test)
def training_pipeline(data_features, data_labels, params):
  X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(data_features, data_labels)
  validation_res = kfold_cross_validation(X_train, y_train, params, k=5)
  #plot_accuracy(validation_res, params, "5-Fold Cross-Validation scores")
  best_param = params[np.argmax(validation_res)]
  #print("best parameter : " + str(best_param))
  #f1_score = evaluate_f1(LogisticRegression(penalty='l1', C=best_param, solver='liblinear', random_state=0), X_train, y_train, X_test, y_test)
  #print("F1 on test set: "+ str(f1_score))
  #acc = evaluate_acc(LogisticRegression(penalty='l1', C=best_param, solver='liblinear', random_state=0), X_train, y_train, X_test, y_test)
  #print("Accuracy on test set: "+ str(acc))
  clf = LogisticRegression(penalty='l1', C=best_param, solver='liblinear', random_state=0)
  clf.fit(data_features, data_labels)
  return clf

# resampling function
def bootstrap_sampling(data_features, data_labels, n_samples=None):
    if n_samples is None:
      n_samples = len(data_labels)
    all_data = np.concatenate((data_features, np.array([data_labels]).T), axis=1)
    sample = resample(all_data, n_samples=n_samples, random_state=None, stratify=None)
    return sample[:,:-1], sample[:,-1]


def plot_accuracy(results, params, title="fill title", label="f1 scores"):
  plt.plot(params, results, '--bo',label=label)
  plt.legend()
  plt.xlabel("Parameter")
  plt.title(title)
  plt.show()


# generate comorbidities, age (binary) and one target symptom
def generate_q1a(df, effect_prob, cause_inds, target_ind, n_cols=8, n_samples=99999):
    data = df.to_numpy()
    sim = Simulator(data)
    res = np.zeros((n_samples, n_cols))
    for i in range(-9,-3):
        res[:,i+9] = sim.generate_binary(i, n_samples)
    age = sim.generate_age(10, n_samples)
    res[:,6:] = preprocess_age(age)
    cause_cols = res[:,cause_inds]
    target_mean = np.mean(data[:,target_ind])
    target = sim.add_effect(cause_cols, target_mean, effect_prob)
    return res, target

# get the difference of symptom frequencies for different effect size and symptoms
def run_simulation_Q1(df, ind=0, n_repeats=5, n_samples=10000):
    probs = np.arange(0.1, 1, 0.2)
    results = np.zeros((5,10,2))
    for i_iter in range(n_repeats):
        for i in range(len(probs)):
            for k in range(10):
                sim_data, symptom = generate_q1a(df, probs[i], cause_inds=ind, target_ind=k, n_samples=n_samples)
                if np.mean(symptom) < 0.05:
                    positive_ind = symptom==1
                    sample_features, sample_labels = bootstrap_sampling(sim_data, symptom, n_samples=int(len(sim_data)/2)) # under sampling
                    sample_features_pos, sample_labels_pos = bootstrap_sampling(sim_data[positive_ind,:], symptom[positive_ind], n_samples=int(len(sim_data)/20)) # over sampling
                    sim_data = np.concatenate((sample_features, sample_features_pos), axis=0)
                    symptom = np.concatenate((sample_labels, sample_labels_pos), axis=0)        
                model = training_pipeline(sim_data, symptom, [0.001, 0.01, 0.1, 0.5, 1, 1.5, 2, 5])
                if model.coef_[0,ind] != 0:
                    results[i,k,0] += 1
                results[i,k,1] += np.sum(model.coef_ != 0)
    return results/n_repeats

# finds significant features for symptom prediction
def Q1_final(df):
    data = df.to_numpy()
    features = data[:,-9:-3]
    age = preprocess_age(data[:,10])
    features = np.concatenate((features, age), axis=1)
    new_features = binary_to_str(features)
    freqItemSet, rules = apriori(new_features, minSup=0.05, minConf=0.5)
    added_features = []
    if 2 in freqItemSet:
        curr_col = np.ones(len(data))
        for s in freqItemSet[2]:
            added_features.append(list(s))
            for val in s:
                curr_col = np.logical_and(curr_col, features[:,val] == 1)
            features = np.concatenate((features, np.array([curr_col]).T), axis=1)
    results = np.zeros((10,len(features[0])))
    
    for i in range(10):
        symptom = data[:,i]
        if np.mean(symptom) < 0.05:
            positive_ind = symptom==1
            sample_features, sample_labels = bootstrap_sampling(features, symptom, n_samples=int(len(data)/2)) # under sampling
            sample_features_pos, sample_labels_pos = bootstrap_sampling(features[positive_ind,:], symptom[positive_ind], n_samples=int(len(data)/20)) # over sampling
            curr_features = np.concatenate((sample_features, sample_features_pos), axis=0)
            curr_symptom = np.concatenate((sample_labels, sample_labels_pos), axis=0)  
        else:
            curr_features = features
            curr_symptom = symptom
        model = training_pipeline(curr_features, curr_symptom, [0.001, 0.01, 0.1, 0.5, 1, 1.5, 2, 5])
        results[i,:] = model.coef_[0]
    return results, added_features

# generate figure 1
def plot_Q1(results, added_features):
    symptoms_names = ['Covid-Recovered', 'Covid-Positive', 'No-Taste/Smell',
                      'Fever', 'Headache', 'Pneumonia',
                      'Stomach', 'Myocarditis', 'Blood-Clots', 'Death']
    labels = ['Asthma', 'Obesity', 'Smoking', 'Diabetes', 'Heart disease', 'Hypertension', 'Age<55', 'Age>=55']
    if added_features:
        for s in added_features:
            curr_name = ""
            for val in s:
                curr_name += str(val) + "+"
            labels.append(curr_name[:-1])
    x = np.arange(len(labels))
    width = 0.3
    fig = plt.figure()
    fig.set_figheight(5)
    fig.set_figwidth(13)
    counter = 1
    n_subplots = np.sum(np.sum(results, axis=1)!=0)
    for i in range(10):
        if np.sum(results[i]) == 0:
            continue
        ax = fig.add_subplot(n_subplots,1,counter)
        ax.bar(np.arange(len(labels)), np.sort(results[i]), width)
        ax.plot(np.arange(len(labels)), np.zeros(len(labels)))
        ax.set_ylabel('Lasso Weight')
        ax.set_title(symptoms_names[i])
        counter+=1
        ax.set_xticks(x)
        ax.set_xticklabels(labels)
    fig.tight_layout()
    plt.show()


def main_q1a(df):
    q1a_res = run_simulation_Q1(df)
    # table 1
    print(q1a_res[:,:,0])
    # table 2
    print(q1a_res[:,:,1])
    results, added_features = Q1_final(df)
    plot_Q1(results, added_features)
    
main_q1a(df)

"""Task 1b (see report)"""

main_Task_1b(df)

""" Functions to compute risk and vaccine efficacy. """
# empirical frequency of covid infections among a specfic group
def compute_risk(group):
    return sum(group[:, 1]) / len(group)


# calculate vaccine efficacy (see definition in report)
def compute_vaccine_efficacy(vaccinated_group, unvaccinated_group):
    return (compute_risk(unvaccinated_group) - compute_risk(vaccinated_group)) / compute_risk(unvaccinated_group)


# Rough estimate of confidence intervals using Hoeffding's inequality.
# N = no. of samples, error_prob = error probability (significance)
def conf_interval_hoeffding(N, error_prob=0.05):
    return math.sqrt(math.log(2 / error_prob) / (2 * N))


# analyse the efficacy of each of the three vaccines; output: plots and some print outs
def main_Task_1b(df):
    # Split the data according to vaccination status.
    vac_status = [147, 148, 149]  # column no. containing the vaccination status
    vac0_group = df.values[np.where(np.all(df.values[:, vac_status] == 0, axis=1))]  # not vaccinated

    # Preparing Plots
    fig, axs = plt.subplots(len(vac_status))
    fig.tight_layout(pad=3.0)
    plt.setp(axs, ylim=(0, 50))
    barWidth = 0.3

    for i in range(len(vac_status)):
        # considering vaccine no. i
        vac_group = df.values[np.where(df.values[:, vac_status[i]] == 1)]
        print("VACCINE NO.", i+1)
        VE = []
        risk_unvaccinated = []
        risk_vaccinated = []
        error_unvaccinated = []
        error_vaccinated = []

        # The vaccine efficacy computed for ALL individuals from the vaccinated and unvaccinated groups
        overall_vaccine_efficacy = compute_vaccine_efficacy(vac_group, vac0_group)
        VE.append(overall_vaccine_efficacy)
        risk_unvaccinated.append(compute_risk(vac0_group))
        risk_vaccinated.append(compute_risk(vac_group))
        error_unvaccinated.append(conf_interval_hoeffding(len(vac0_group)))
        error_vaccinated.append(conf_interval_hoeffding(len(vac_group)))
        print("Overall Vaccine Efficacy of Vaccine ", i + 1, round(100 * overall_vaccine_efficacy, 2), "%")
        # print("Vac", i+1, sum(vac_group[:, 1]), "/", len(vac_group))
        # print("UnVac", i+1, sum(vac0_group[:, 1]), "/", len(vac0_group))

        """ Adjusting the analysis for Age (<55 vs >54) and Gender (f vs m). """
        # age adjusted groups
        index_age = 10
        age_adj_vac0 = vac0_group[np.where(vac0_group[:, index_age] < 55)]
        age_adj_vac = vac_group[np.where(vac_group[:, index_age] < 55)]
        age_adj_54 = compute_vaccine_efficacy(age_adj_vac, age_adj_vac0)
        VE.append(age_adj_54)
        risk_unvaccinated.append(compute_risk(age_adj_vac0))
        risk_vaccinated.append(compute_risk(age_adj_vac))
        error_unvaccinated.append(conf_interval_hoeffding(len(age_adj_vac0)))
        error_vaccinated.append(conf_interval_hoeffding(len(age_adj_vac)))
        print("Vaccine Efficacy for Vaccine", i + 1, "for Age < 55:", round(100 * age_adj_54, 2), "%")
        # print("Vac", i+1, sum(age_adj_vac[:, 1]), "/", len(age_adj_vac))
        # print("UnVac", i+1, sum(age_adj_vac0[:, 1]), "/", len(age_adj_vac0))

        # >= 55 years old
        age_adj_vac0 = vac0_group[np.where(vac0_group[:, index_age] > 54)]
        age_adj_vac = vac_group[np.where(vac_group[:, index_age] > 54)]
        age_adj_55 = compute_vaccine_efficacy(age_adj_vac, age_adj_vac0)
        VE.append(age_adj_55)
        risk_unvaccinated.append(compute_risk(age_adj_vac0))
        risk_vaccinated.append(compute_risk(age_adj_vac))
        error_unvaccinated.append(conf_interval_hoeffding(len(age_adj_vac0)))
        error_vaccinated.append(conf_interval_hoeffding(len(age_adj_vac)))
        print("Vaccine Efficacy for Vaccine", i + 1, "for Age >= 55:", round(100 * age_adj_55, 2), "%")
        # print("Vac", i+1, sum(age_adj_vac[:, 1]), "/", len(age_adj_vac))
        # print("UnVac", i+1, sum(age_adj_vac0[:, 1]), "/", len(age_adj_vac0))

        # gender adjusted groups
        index_sex = 11
        sex_adj_vac0 = vac0_group[np.where(vac0_group[:, index_sex] == 0)]
        sex_adj_vac = vac_group[np.where(vac_group[:, index_sex] == 0)]
        sex_adj_female = compute_vaccine_efficacy(sex_adj_vac, sex_adj_vac0)
        VE.append(sex_adj_female)
        risk_unvaccinated.append(compute_risk(sex_adj_vac0))
        risk_vaccinated.append(compute_risk(sex_adj_vac))
        error_unvaccinated.append(conf_interval_hoeffding(len(sex_adj_vac0)))
        error_vaccinated.append(conf_interval_hoeffding(len(sex_adj_vac)))
        print("Vaccine Efficacy for Vaccine", i + 1, "for female patients:", round(100 * sex_adj_female, 2), "%")
        # print("Vac", i+1, sum(sex_adj_vac[:, 1]), "/", len(sex_adj_vac))
        # print("UnVac", i+1, sum(sex_adj_vac0[:, 1]), "/", len(sex_adj_vac0))

        # male
        sex_adj_vac0 = vac0_group[np.where(vac0_group[:, index_sex] == 0)]
        sex_adj_vac = vac_group[np.where(vac_group[:, index_sex] == 0)]
        sex_adj_male = compute_vaccine_efficacy(sex_adj_vac, sex_adj_vac0)
        VE.append(sex_adj_male)
        risk_unvaccinated.append(compute_risk(sex_adj_vac0))
        risk_vaccinated.append(compute_risk(sex_adj_vac))
        error_unvaccinated.append(conf_interval_hoeffding(len(sex_adj_vac0)))
        error_vaccinated.append(conf_interval_hoeffding(len(sex_adj_vac)))
        print("Vaccine Efficacy for Vaccine", i + 1, "for male patients:", round(100 * sex_adj_male, 2), "%")
        # print("Vac", i+1, sum(sex_adj_vac[:, 1]), "/", len(sex_adj_vac))
        # print("UnVac", i+1, sum(sex_adj_vac0[:, 1]), "/", len(sex_adj_vac0))

        # Plotting
        VE = np.array(VE) * 100  # converting to percentages
        risk_unvaccinated = np.array(risk_unvaccinated) * 100
        risk_vaccinated = np.array(risk_vaccinated) * 100
        error_unvaccinated = np.array(error_unvaccinated) * 100
        error_vaccinated = np.array(error_vaccinated) * 100
        ticks = np.arange(5)
        plt.sca(axs[i])
        plt.xticks(ticks + barWidth, ["Overall", "<55", ">=55", "female", "male"])
        axs[i].bar(x=ticks, height=VE, width=barWidth, label='Vaccine Efficacy')
        axs[i].bar(x=ticks + barWidth, height=risk_unvaccinated, width=barWidth, yerr=error_unvaccinated, label='Risk Unvaccinated')
        axs[i].bar(x=ticks + 2 * barWidth, height=risk_vaccinated, width=barWidth, yerr=error_vaccinated, label='Risk Vaccinated')
        axs[i].set_ylabel("Percentages %", fontsize=7)
        axs[i].title.set_text("Vaccine No. " + str(i + 1))
        axs[i].legend(loc='upper center', bbox_to_anchor=(0.5, 1.02), fancybox=True, shadow=False, ncol=5, fontsize=7)
        plt.savefig("vaccine_efficacy_and_risk.pdf")
    plt.show()

"""Task 1c (see report)"""

def main_Task_1c(df):
    # Prepare the data and call the sub functions
    slc = [0,1,2,3,4,5,6,7,8,9,147,148,149]
    df = df.iloc[:,slc]
    df.columns = ['Covid-Recovered', 'Covid-Positive', 'No-Taste/Smell', 'Fever', 'Headache', 'Pneumonia', 'Stomach', 'Myocarditis', 'Blood-Clots', 'Death', 'vac1','vac2','vac3']
    df = df.loc[(df['Covid-Recovered'] == 0) & (df['Covid-Positive'] == 0)]
    df4 = df.drop(columns=['Covid-Recovered', 'Covid-Positive'])
    Logistic = Pr_log_reg_all(df4)
    
    Pr_logistic = pd.DataFrame(Logistic[0], columns = ['No-Taste/Smell', 'Fever', 'Headache', 'Pneumonia', 'Stomach', 'Myocarditis', 'Blood-Clots', 'Death'], index = ['Vaccine 1', 'Vaccine 2', 'Vaccine 3'])
    CI_logistic_lower = pd.DataFrame(Logistic[1], columns = ['No-Taste/Smell', 'Fever', 'Headache', 'Pneumonia', 'Stomach', 'Myocarditis', 'Blood-Clots', 'Death'], index = ['Vaccine 1', 'Vaccine 2', 'Vaccine 3'])
    CI_logistic_upper = pd.DataFrame(Logistic[2], columns = ['No-Taste/Smell', 'Fever', 'Headache', 'Pneumonia', 'Stomach', 'Myocarditis', 'Blood-Clots', 'Death'], index = ['Vaccine 1', 'Vaccine 2', 'Vaccine 3'])
    
    print("Probabilities:")
    display(Pr_logistic)
    print("CI lower:")
    display(CI_logistic_lower)
    print("CI upper:")
    display(CI_logistic_upper)

def log_reg(side_effect, df4):
    # Estimate the probabilities with confidence intervals of the given side-effects for all vaccines
    df_no_vaccine2 = df4.loc[(df4['vac1'] == 0) & (df4['vac2'] == 0) & (df4['vac3'] == 0)]
    df_only_vac1 = df4.loc[(df4['vac1'] == 1)]
    df_only_vac1 = df_only_vac1.reset_index(drop=True)

    df_only_vac2 = df4.loc[(df4['vac2'] == 1)]
    df_only_vac2 = df_only_vac2.reset_index(drop=True)

    df_only_vac3 = df4.loc[(df4['vac3'] == 1)]
    df_only_vac3 = df_only_vac3.reset_index(drop=True)
        
    pred_vac = np.zeros((3,3))
    i = 0
    for df_vac, vac  in zip([df_only_vac1, df_only_vac2, df_only_vac3],['vac1','vac2','vac3']):
        
        if ((df4[side_effect]==1) & (df4[vac]==1)).sum() == 0:
            pred_vac[i,:] = 0
        
        else:
            y = df_vac[[side_effect]].to_numpy()
            X = df_vac[[vac]].to_numpy()
            X = add_constant(X)

            model = sm.Logit(y, X).fit(disp=0)
            pred_vac[i,0] = model.predict([1])
            test = model.summary()

            se = np.sqrt(model.cov_params())[0]
            pred_vac[i,1] = expit(logit(pred_vac[i,0]) - 1.96*se)
            pred_vac[i,2] = expit(logit(pred_vac[i,0]) + 1.96*se)
            
            i+=1
    return pred_vac*100

def Pr_log_reg_all(df4):
    # Run through all side-effects
    Pr_diff = np.zeros((3,8))
    CI_lower = np.zeros((3,8))
    CI_upper = np.zeros((3,8))

    col = 0
    for side_effect in ['No-Taste/Smell', 'Fever', 'Headache', 'Pneumonia', 'Stomach', 'Myocarditis', 'Blood-Clots', 'Death']:
        Pr_diff[:,col] = log_reg(side_effect, df4)[:,0]
        CI_lower[:,col] = log_reg(side_effect, df4)[:,1]
        CI_upper[:,col] = log_reg(side_effect, df4)[:,2]
        col +=1

    return Pr_diff, CI_lower, CI_upper

main_Task_1c(df)

df_features = pd.read_csv('/treatment_features.csv')
df_action = pd.read_csv('/treatment_actions.csv')
df_outcome = pd.read_csv('/treatment_outcomes.csv')

# generate all features and one target symptom (based on treatment)
def generate_q2(df_features, df_action, df_outcome, effect_prob1, effect_prob2, target_ind, n_samples=877, balance=True):
    data = df_features.to_numpy()
    treatment = df_action.to_numpy()
    targets = df_outcome.to_numpy()
    sim = Simulator(data)
    sim2 = Simulator(treatment)
    res = np.zeros((n_samples, len(data[0])+2))
    for col in range(len(data[0])):
        if col == 10:
            res[:,col] = sim.generate_age(col, n_samples)
        elif col == 12:
            res[:,col] = sim.generate_income(col, n_samples)
        else:
            res[:,col] = sim.generate_binary(col, n_samples)
    res[:,-2] = sim2.generate_binary(0, n_samples)
    res[:,-1] = sim2.generate_binary(1, n_samples)
    cause_cols = res[:,[-2,-1]]
    target_mean = np.mean(targets[:,target_ind])
    symptom = sim.treatment_effect(cause_cols, target_mean, effect_prob1*target_mean, effect_prob2*target_mean)
    if balance:
        positive_ind = symptom==1
        while np.sum(positive_ind) == 0:
            symptom = sim.treatment_effect(cause_cols, target_mean, effect_prob1*target_mean, effect_prob2*target_mean)
        sample_features, sample_labels = bootstrap_sampling(res, symptom, n_samples=int(n_samples/2)) # under sampling
        sample_features_pos, sample_labels_pos = bootstrap_sampling(res[positive_ind,:], symptom[positive_ind], n_samples=int(n_samples/20)) # over sampling
        res = np.concatenate((sample_features, sample_features_pos), axis=0)
        symptom = np.concatenate((sample_labels, sample_labels_pos), axis=0)
    return res, symptom
    



# simulate different treatment effects
def run_simulation_Q2(df_features, df_action, df_outcome, n_repeats=5, n_samples=10000):
    probs1 = np.arange(0.1, 1, 0.3)
    probs2 = np.arange(0.1, 1, 0.3)
    results = np.zeros((3,3,8,4))
    for i_iter in range(n_repeats):
        for i in range(len(probs1)):
            for j in range(len(probs2)):
                for k in range(-8,0):
                    sim_data, symptom = generate_q2(df_features, df_action, df_outcome, probs1[i], probs2[j], k, n_samples=n_samples, balance=True)
                    model = LogisticRegression(penalty='l1', C=0.5, solver='liblinear', random_state=0).fit(sim_data, symptom)
                    sim_test_true = sim_test.copy()
                    sim_test[:,-2] = np.zeros(len(sim_test[:,-2]))
                    sim_test[:,-1] = np.zeros(len(sim_test[:,-1]))
                    true_0 = np.mean(symptom_test[np.where(np.logical_and(sim_test_true[:,-2]==0, sim_test_true[:,-1]==0))])
                    results[i,j,k,0] += np.absolute(np.mean(model.predict(sim_test)) - true_0)
                    sim_test[:,-2] = np.ones(len(sim_test[:,-2]))
                    true_1 = np.mean(symptom_test[np.where(np.logical_and(sim_test_true[:,-2]==1, sim_test_true[:,-1]==0))])
                    results[i,j,k,1] += np.absolute(np.mean(model.predict(sim_test)) - true_1)
                    sim_test[:,-2] = np.zeros(len(sim_test[:,-2]))
                    sim_test[:,-1] = np.ones(len(sim_test[:,-1]))
                    true_2 = np.mean(symptom_test[np.where(np.logical_and(sim_test_true[:,-2]==0, sim_test_true[:,-1]==1))])
                    results[i,j,k,2] += np.absolute(np.mean(model.predict(sim_test)) - true_2)
                    sim_test[:,-2] = np.ones(len(sim_test[:,-2]))
                    sim_test[:,-1] = np.ones(len(sim_test[:,-1]))
                    true_3 = np.mean(symptom_test[np.where(np.logical_and(sim_test_true[:,-2]==1, sim_test_true[:,-1]==1))])
                    results[i,j,k,3] += np.absolute(np.mean(model.predict(sim_test)) - true_3)
    return results/n_repeats

# generates figure 4
def plot_bars_simulations_Q2(df_outcome, results, effects=np.arange(0.1, 1, 0.3)):
    symptoms_names = ['No-Taste/Smell', 'Fever', 'Headache', 'Pneumonia',
                      'Stomach', 'Myocarditis', 'Blood-Clots', 'Death']
    targets = df_outcome.to_numpy()
    real_means = np.mean(targets, axis=0)
    labels = ['No Treatment', 'Treatment1', 'Treatment2', 'Both Treatments']
    x = np.arange(len(labels))
    width = 0.1
    fig = plt.figure()
    fig.set_figheight(10)
    fig.set_figwidth(15)
    counter = 1
    for ind1 in range(len(effects)):
        for ind2 in range(len(effects)):
            curr_results = results[ind1, ind2]
            rects = []
            ax = fig.add_subplot(len(effects),len(effects),counter)
            counter+=1
            for i in range(8):
                if i < 4:
                    location = x - (width)*(4-i)
                else:
                    location = x + (width)*(i-4)
                rects.append(ax.bar(location, curr_results[i]/real_means[i], width, label=symptoms_names[i]))
            ax.set_ylabel('Relative Frequency Error')
            ax.set_title('Effect1 = '+str(np.round(effects[ind1],1)) + ', effect2 = '+str(np.round(effects[ind2],1)))
            ax.set_xticks(x)
            ax.set_xticklabels(labels)
            ax.legend()
    fig.tight_layout()
    plt.show()

# trains a model of the effect of treatment on a given symptom (ind)
# get prediction probabilities for specific samples (test_ind)
def Q2_get_model(df_features, df_action, df_outcome, ind, test_ind, balance=True):
    data = df_features.to_numpy()
    treatment = df_action.to_numpy()
    symptom = df_outcome.to_numpy()[:,ind]
    features = np.concatenate((data, treatment), axis=1)
    if balance:
        positive_ind = symptom==1
        sample_features, sample_labels = bootstrap_sampling(features, symptom, n_samples=int(len(data)/2)) # under sampling
        sample_features_pos, sample_labels_pos = bootstrap_sampling(features[positive_ind,:], symptom[positive_ind], n_samples=int(len(data)/20)) # over sampling
        features = np.concatenate((sample_features, sample_features_pos), axis=0)
        symptom = np.concatenate((sample_labels, sample_labels_pos), axis=0)        
    model = training_pipeline(features, symptom, [0.001, 0.01, 0.1, 0.5, 1, 1.5, 2, 5])
    results = np.zeros((len(test_ind),4))
    for i,val in enumerate(test_ind):
        results[i,0] = model.predict_proba([np.concatenate((data[val], np.array([0,0])))])[0][1]
        results[i,1] = model.predict_proba([np.concatenate((data[val], np.array([1,0])))])[0][1]
        results[i,2] = model.predict_proba([np.concatenate((data[val], np.array([0,1])))])[0][1]
        results[i,3] = model.predict_proba([np.concatenate((data[val], np.array([1,1])))])[0][1]
    return model, results

# generates figure 5
def plot_bars_final_Q2(results):
    symptoms_names = ['No-Taste/Smell', 'Fever', 'Headache', 'Pneumonia',
                      'Stomach', 'Myocarditis', 'Blood-Clots', 'Death']
    labels = ['No Treatment', 'Treatment1', 'Treatment2', 'Both Treatments']
    x = np.arange(len(labels))
    width = 0.1
    fig = plt.figure()
    fig.set_figheight(3)
    fig.set_figwidth(15)
    counter = 1
    for ind1 in range(len(results[0])):
        curr_results = results[:, ind1,:]
        rects = []
        ax = fig.add_subplot(1,len(results[0]),counter)
        for i in range(8):
            if i < 4:
                location = x - (width)*(4-i)
            else:
                location = x + (width)*(i-4)
            rects.append(ax.bar(location, curr_results[i], width, label=symptoms_names[i]))        
        ax.set_ylabel('Predicted Probability')
        ax.set_title("Example "+ str(counter))
        counter+=1
        ax.set_xticks(x)
        ax.set_xticklabels(labels)
        ax.legend()
    fig.tight_layout()
    plt.show()

def main_Q2(df_features, df_action, df_outcome):  
    results = run_simulation_Q2(df_features, df_action, df_outcome, n_repeats=5, n_samples=10000)
    plot_bars_simulations_Q2(df_outcome, results, effects=np.arange(0.1, 1, 0.3))
    final_Q2 = np.zeros((8,3,4))
    for ind in range(-8,0):
        curr_model, curr_res = Q2_get_model(df_features, df_action, df_outcome, 
                                            ind, [0,2,3], balance=True)
        final_Q2[ind] = curr_res
    plot_bars_final_Q2(final_Q2)

main_Q2(df_features, df_action, df_outcome)